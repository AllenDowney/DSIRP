{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-supplier",
   "metadata": {},
   "source": [
    "# Chapter xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-morrison",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "*Data Structures and Information Retrieval in Python*\n",
    "\n",
    "Copyright 2021 Allen Downey\n",
    "\n",
    "License: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "level-chaos",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print('Downloaded ' + local)\n",
    "    \n",
    "# download('https://github.com/AllenDowney/DSIRP/raw/main/utils.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-great",
   "metadata": {},
   "source": [
    "[Click here to run this chapter on Colab](https://colab.research.google.com/github/AllenDowney/DSIRP/blob/main/chapters/chap01.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-button",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "russian-drain",
   "metadata": {},
   "source": [
    "# Crawling Wikipedia\n",
    "\n",
    "In this chapter, I present a solution to the previous exercise and\n",
    "analyze the performance of Web indexing algorithms. Then we build a\n",
    "simple Web crawler.\n",
    "\n",
    "## The Redis-backed indexer\n",
    "\n",
    "In my solution, we store two kinds of structures in Redis:\n",
    "\n",
    "-   For each search term, we have a `URLSet`, which is a Redis of URLs\n",
    "    that contain the search term.\n",
    "\n",
    "-   For each URL, we have a `TermCounter`, which is a Redis that maps\n",
    "    each search term to the number of times it appears.\n",
    "\n",
    "We discussed these data types in the previous chapter. You can also read\n",
    "about Redis structures at <http://thinkdast.com/redistypes>\n",
    "\n",
    "In `JedisIndex`, I provide a method that takes a search term and returns\n",
    "the Redis key of its `URLSet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "private String urlSetKey(String term) {\n",
    "    return \"URLSet:\" + term;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-worst",
   "metadata": {},
   "source": [
    "And a method that takes a URL and returns the Redis key of its\n",
    "`TermCounter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "private String termCounterKey(String url) {\n",
    "    return \"TermCounter:\" + url;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-rental",
   "metadata": {},
   "source": [
    "Here's the implementation of `indexPage`, which takes a URL and a jsoup\n",
    "`Elements` object that contains the DOM tree of the paragraphs we want\n",
    "to index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "public void indexPage(String url, Elements paragraphs) {\n",
    "    System.out.println(\"Indexing \" + url);\n",
    "\n",
    "    // make a TermCounter and count the terms in the paragraphs\n",
    "    TermCounter tc = new TermCounter(url);\n",
    "    tc.processElements(paragraphs);\n",
    "\n",
    "    // push the contents of the TermCounter to Redis\n",
    "    pushTermCounterToRedis(tc);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-lightning",
   "metadata": {},
   "source": [
    "To index a page, we\n",
    "\n",
    "1.  Make a Java `TermCounter` for the contents of the page, using code\n",
    "    from a previous exercise.\n",
    "\n",
    "2.  Push the contents of the `TermCounter` to Redis.\n",
    "\n",
    "Here's the new code that pushes a `TermCounter` to Redis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "public List<Object> pushTermCounterToRedis(TermCounter tc) {\n",
    "    Transaction t = jedis.multi();\n",
    "\n",
    "    String url = tc.getLabel();\n",
    "    String hashname = termCounterKey(url);\n",
    "\n",
    "    // if this page has already been indexed, delete the old hash\n",
    "    t.del(hashname);\n",
    "\n",
    "    // for each term, add an entry in the TermCounter and a new\n",
    "    // member of the index\n",
    "    for (String term: tc.keySet()) {\n",
    "        Integer count = tc.get(term);\n",
    "        t.hset(hashname, term, count.toString());\n",
    "        t.sadd(urlSetKey(term), url);\n",
    "    }\n",
    "    List<Object> res = t.exec();\n",
    "    return res;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-glucose",
   "metadata": {},
   "source": [
    "This method uses a `Transaction` to collect the operations and send them\n",
    "to the server all at once, which is much faster than sending a series of\n",
    "small operations.\n",
    "\n",
    "It loops through the terms in the `TermCounter`. For each one it\n",
    "\n",
    "1.  Finds or creates a `TermCounter` on Redis, then adds a field for the\n",
    "    new term.\n",
    "\n",
    "2.  Finds or creates a `URLSet` on Redis, then adds the current URL.\n",
    "\n",
    "If the page has already been indexed, we delete its old `TermCounter`\n",
    "before pushing the new contents.\n",
    "\n",
    "That's it for indexing new pages.\n",
    "\n",
    "The second part of the exercise asked you to write `getCounts`, which\n",
    "takes a search term and returns a map from each URL where the term\n",
    "appears to the number of times it appears there. Here is my solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "public Map<String, Integer> getCounts(String term) {\n",
    "    Map<String, Integer> map = new HashMap<String, Integer>();\n",
    "    Set<String> urls = getURLs(term);\n",
    "    for (String url: urls) {\n",
    "        Integer count = getCount(url, term);\n",
    "        map.put(url, count);\n",
    "    }\n",
    "    return map;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-antique",
   "metadata": {},
   "source": [
    "This method uses two helper methods:\n",
    "\n",
    "-   `getURLs` takes a search term and returns the Set of URLs where the\n",
    "    term appears.\n",
    "\n",
    "-   `getCount` takes a URL and a term and returns the number of times\n",
    "    the term appears at the given URL.\n",
    "\n",
    "Here are the implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "public Set<String> getURLs(String term) {\n",
    "    Set<String> set = jedis.smembers(urlSetKey(term));\n",
    "    return set;\n",
    "}\n",
    "\n",
    "public Integer getCount(String url, String term) {\n",
    "    String redisKey = termCounterKey(url);\n",
    "    String count = jedis.hget(redisKey, term);\n",
    "    return new Integer(count);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-wyoming",
   "metadata": {},
   "source": [
    "Because of the way we designed the index, these methods are simple and\n",
    "efficient.\n",
    "\n",
    "## Analysis of lookup\n",
    "\n",
    "Suppose we have indexed $N$ pages and discovered $M$ unique search\n",
    "terms. How long will it take to look up a search term? Think about your\n",
    "answer before you continue.\n",
    "\n",
    "To look up a search term, we run `getCounts`, which\n",
    "\n",
    "1.  Creates a map.\n",
    "\n",
    "2.  Runs `getURLs` to get a Set of URLs.\n",
    "\n",
    "3.  For each URL in the Set, runs `getCount` and adds an entry to a\n",
    "    `HashMap`.\n",
    "\n",
    "`getURLs` takes time proportional to the number of URLs that contain the\n",
    "search term. For rare terms, that might be a small number, but for\n",
    "common terms it might be as large as $N$.\n",
    "\n",
    "Inside the loop, we run `getCount`, which finds a `TermCounter` on\n",
    "Redis, looks up a term, and adds an entry to a HashMap. Those are all\n",
    "constant time operations, so the overall complexity of `getCounts` is\n",
    "$O(N)$ in the worst case. However, in practice the runtime is\n",
    "proportional to the number of pages that contain the term, which is\n",
    "normally much less than $N$.\n",
    "\n",
    "This algorithm is as efficient as it can be, in terms of algorithmic\n",
    "complexity, but it is very slow because it sends many small operations\n",
    "to Redis. You can make it faster using a `Transaction`. You might want\n",
    "to do that as an exercise, or you can see my solution in\n",
    "`RedisIndex.java`.\n",
    "\n",
    "## Analysis of indexing\n",
    "\n",
    "Using the data structures we designed, how long will it take to index a\n",
    "page? Again, think about your answer before you continue.\n",
    "\n",
    "To index a page, we traverse its DOM tree, find all the `TextNode`\n",
    "objects, and split up the strings into search terms. That all takes time\n",
    "proportional to the number of words on the page.\n",
    "\n",
    "For each term, we increment a counter in a HashMap, which is a constant\n",
    "time operation. So making the `TermCounter` takes time proportional to\n",
    "the number of words on the page.\n",
    "\n",
    "Pushing the `TermCounter` to Redis requires deleting a `TermCounter`,\n",
    "which is linear in the number of unique terms. Then for each term we\n",
    "have to\n",
    "\n",
    "1.  Add an element to a `URLSet`, and\n",
    "\n",
    "2.  Add an element to a Redis `TermCounter`.\n",
    "\n",
    "Both of these are constant time operations, so the total time to push\n",
    "the `TermCounter` is linear in the number of unique search terms.\n",
    "\n",
    "In summary, making the `TermCounter` is proportional to the number of\n",
    "words on the page. Pushing the `TermCounter` to Redis is proportional to\n",
    "the number of unique terms.\n",
    "\n",
    "Since the number of words on the page usually exceeds the number of\n",
    "unique search terms, the overall complexity is proportional to the\n",
    "number of words on the page. In theory a page might contain all search\n",
    "terms in the index, so the worst case performance is $O(M)$, but we\n",
    "don't expect to see the worse case in practice.\n",
    "\n",
    "This analysis suggests a way to improve performance: we should probably\n",
    "avoid indexing very common words. First of all, they take up a lot of\n",
    "time and space, because they appear in almost every `URLSet` and\n",
    "`TermCounter`. Furthermore, they are not very useful because they don't\n",
    "help identify relevant pages.\n",
    "\n",
    "Most search engines avoid indexing common words, which are known in this\n",
    "context as stop words (<http://thinkdast.com/stopword>).\n",
    "\n",
    "## Graph traversal\n",
    "\n",
    "If you did the \"Getting to Philosophy\" exercise in\n",
    "Chapter [\\[getphilo\\]](#getphilo){reference-type=\"ref\"\n",
    "reference=\"getphilo\"}, you already have a program that reads a Wikipedia\n",
    "page, finds the first link, uses the link to load the next page, and\n",
    "repeats. This program is a specialized kind of crawler, but when people\n",
    "say \"Web crawler\" they usually mean a program that\n",
    "\n",
    "-   Loads a starting page and indexes the contents,\n",
    "\n",
    "-   Finds all the links on the page and adds the linked URLs to a\n",
    "    collection, and\n",
    "\n",
    "-   Works its way through the collection, loading pages, indexing them,\n",
    "    and adding new URLs.\n",
    "\n",
    "-   If it finds a URL that has already been indexed, it skips it.\n",
    "\n",
    "You can think of the Web as a graph where each page is a node and each\n",
    "link is a directed edge from one node to another. If you are not\n",
    "familiar with graphs, you can read about them at\n",
    "<http://thinkdast.com/graph>.\n",
    "\n",
    "Starting from a source node, a crawler traverses this graph, visiting\n",
    "each reachable node once.\n",
    "\n",
    "The collection we use to store the URLs determines what kind of\n",
    "traversal the crawler performs:\n",
    "\n",
    "-   If it's a first-in-first-out (FIFO) queue, the crawler performs a\n",
    "    breadth-first traversal.\n",
    "\n",
    "-   If it's a last-in-first-out (LIFO) stack, the crawler performs a\n",
    "    depth-first traversal.\n",
    "\n",
    "-   More generally, the items in the collection might be prioritized.\n",
    "    For example, we might want to give higher priority to pages that\n",
    "    have not been indexed for a long time.\n",
    "\n",
    "You can read more about graph traversal at\n",
    "<http://thinkdast.com/graphtrav>.\n",
    "\n",
    "## Exercise 12\n",
    "\n",
    "Now it's time to write the crawler. In the repository for this book,\n",
    "you'll find the source files for this exercise:\n",
    "\n",
    "-   `WikiCrawler.java`, which contains starter code for your crawler.\n",
    "\n",
    "-   `WikiCrawlerTest.java`, which contains test code for `WikiCrawler`.\n",
    "\n",
    "-   `JedisIndex.java`, which is my solution to the previous exercise.\n",
    "\n",
    "You'll also need some of the helper classes we've used in previous\n",
    "exercises:\n",
    "\n",
    "-   `JedisMaker.java`\n",
    "\n",
    "-   `WikiFetcher.java`\n",
    "\n",
    "-   `TermCounter.java`\n",
    "\n",
    "-   `WikiNodeIterable.java`\n",
    "\n",
    "Before you run `JedisMaker`, you have to provide a file with information\n",
    "about your Redis server. If you did this in the previous exercise, you\n",
    "should be all set. Otherwise you can find instructions in\n",
    "Section [\\[hello-jedis\\]](#hello-jedis){reference-type=\"ref\"\n",
    "reference=\"hello-jedis\"}.\n",
    "\n",
    "Run `ant build` to compile the source files, then run `ant JedisMaker`\n",
    "to make sure it is configured to connect to your Redis server.\n",
    "\n",
    "Now run `ant WikiCrawlerTest`. It should fail, because you have work to\n",
    "do!\n",
    "\n",
    "Here's the beginning of the `WikiCrawler` class I provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "public class WikiCrawler {\n",
    "\n",
    "    public final String source;\n",
    "    private JedisIndex index;\n",
    "    private Queue<String> queue = new LinkedList<String>();\n",
    "    final static WikiFetcher wf = new WikiFetcher();\n",
    "\n",
    "    public WikiCrawler(String source, JedisIndex index) {\n",
    "        this.source = source;\n",
    "        this.index = index;\n",
    "        queue.offer(source);\n",
    "    }\n",
    "\n",
    "    public int queueSize() {\n",
    "        return queue.size();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-champion",
   "metadata": {},
   "source": [
    "The instance variables are\n",
    "\n",
    "-   `source` is the URL where we start crawling.\n",
    "\n",
    "-   `index` is the `JedisIndex` where the results should go.\n",
    "\n",
    "-   `queue` is a `LinkedList` where we keep track of URLs that have been\n",
    "    discovered but not yet indexed.\n",
    "\n",
    "-   `wf` is the `WikiFetcher` we'll use to read and parse Web pages.\n",
    "\n",
    "Your job is to fill in `crawl`. Here's the prototype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "public String crawl(boolean testing) throws IOException {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-payment",
   "metadata": {},
   "source": [
    "The parameter `testing` will be `true` when this method is called from\n",
    "`WikiCrawlerTest` and should be `false` otherwise.\n",
    "\n",
    "When `testing` is `true`, the `crawl` method should:\n",
    "\n",
    "-   Choose and remove a URL from the queue in FIFO order.\n",
    "\n",
    "-   Read the contents of the page using `WikiFetcher.readWikipedia`,\n",
    "    which reads cached copies of pages included in the repository for\n",
    "    testing purposes (to avoid problems if the Wikipedia version\n",
    "    changes).\n",
    "\n",
    "-   It should index pages regardless of whether they are already\n",
    "    indexed.\n",
    "\n",
    "-   It should find all the internal links on the page and add them to\n",
    "    the queue in the order they appear. \"Internal links\" are links to\n",
    "    other Wikipedia pages.\n",
    "\n",
    "-   And it should return the URL of the page it indexed.\n",
    "\n",
    "When `testing` is `false`, this method should:\n",
    "\n",
    "-   Choose and remove a URL from the queue in FIFO order.\n",
    "\n",
    "-   If the URL is already indexed, it should not index it again, and\n",
    "    should return `null`.\n",
    "\n",
    "-   Otherwise it should read the contents of the page using\n",
    "    `WikiFetcher.fetchWikipedia`, which reads current content from the\n",
    "    Web.\n",
    "\n",
    "-   Then it should index the page, add links to the queue, and return\n",
    "    the URL of the page it indexed.\n",
    "\n",
    "`WikiCrawlerTest` loads the queue with about 200 links and then invokes\n",
    "`crawl` three times. After each invocation, it checks the return value\n",
    "and the new length of the queue.\n",
    "\n",
    "When your crawler is working as specified, this test should pass. Good\n",
    "luck!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
